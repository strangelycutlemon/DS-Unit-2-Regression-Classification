{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "DS7_lesson_regression_classification_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV7t5CblkDof",
        "colab_type": "text"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 3*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBMcrliJ0UWA",
        "colab_type": "text"
      },
      "source": [
        "# Regression & Classification, Module 3\n",
        "- Make visualizations to explore relationships between features and target\n",
        "- Do feature selection\n",
        "- Do one-hot encoding of categorical features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmve7Rw70UWC",
        "colab_type": "text"
      },
      "source": [
        "### Setup\n",
        "\n",
        "You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab (run the code cell below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-T3HbUZ0UWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "# If you're in Colab...\n",
        "if in_colab:\n",
        "    # Pull files from Github repo\n",
        "    os.chdir('/content')\n",
        "    !git init .\n",
        "    !git remote add origin https://github.com/LambdaSchool/DS-Unit-2-Regression-Classification.git\n",
        "    !git pull origin master\n",
        "    \n",
        "    # Install required python packages\n",
        "    !pip install -r requirements.txt\n",
        "    \n",
        "    # Change into directory for module\n",
        "    os.chdir('module3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_v4SKV60UWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore this Numpy warning when using Plotly Express:\n",
        "# FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKVeRZX7s1CY",
        "colab_type": "text"
      },
      "source": [
        "# Make visualizations to explore relationships between features and target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21buFx61s3dE",
        "colab_type": "text"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Let's start with some example solutions from yesterday's assingment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2GizMlE0UWI",
        "colab_type": "text"
      },
      "source": [
        "First, load data & remove outliers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnfqBhUX0UWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read New York City apartment rental listing data\n",
        "df = pd.read_csv('../data/renthop-nyc.csv')\n",
        "assert df.shape == (49352, 34)\n",
        "\n",
        "# Remove the most extreme 1% prices,\n",
        "# the most extreme .1% latitudes, &\n",
        "# the most extreme .1% longitudes\n",
        "df = df[(df['price'] >= np.percentile(df['price'], 0.5)) & \n",
        "        (df['price'] <= np.percentile(df['price'], 99.5)) & \n",
        "        (df['latitude'] >= np.percentile(df['latitude'], 0.05)) & \n",
        "        (df['latitude'] < np.percentile(df['latitude'], 99.95)) &\n",
        "        (df['longitude'] >= np.percentile(df['longitude'], 0.05)) & \n",
        "        (df['longitude'] <= np.percentile(df['longitude'], 99.95))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI1Wjv1p0UWL",
        "colab_type": "text"
      },
      "source": [
        "Do train/test split\n",
        "\n",
        "- Use data from April & May 2016 to train\n",
        "- Use data from June 2016 to test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lrMDTO50UWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to datetime and look at the date range\n",
        "df['created'] = pd.to_datetime(df['created'], infer_datetime_format=True)\n",
        "df['created'].describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wCBaH9h0UWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are 16217 + 15627 observations in April & May 2016,\n",
        "# and 16973 observations in June 2016.\n",
        "df['created'].dt.month.value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgmqEuj50UWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are many ways to do train/test split based on date.\n",
        "# Here's one way:\n",
        "train = df[df.created.dt.month < 6]\n",
        "test  = df[df.created.dt.month == 6]\n",
        "train.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRk4RUDr0UWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's another way\n",
        "cutoff = pd.to_datetime('2016-06-01')\n",
        "train = df[df.created < cutoff]\n",
        "test  = df[df.created >= cutoff]\n",
        "train.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLv8wrnl0UWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Here's another way\n",
        "df = df.set_index('created')\n",
        "train = df[:'2016-05'].reset_index()\n",
        "test  = df['2016-06':].reset_index()\n",
        "df = df.reset_index()\n",
        "train.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsBhN4XS0UWX",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "\n",
        "\"Location, Location, Location\" is a real estate slogan.\n",
        "\n",
        "Let's explore the relationship between location & price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STxg_O2R0UWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import plotly.express as px\n",
        "px.scatter(train, x='longitude', y='latitude', color='price')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-bGLVBZtaD0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "px.scatter(train, x='longitude', y='price', trendline='ols', opacity=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGlgd-enuBAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "px.scatter(train, x='latitude', y='price', trendline='ols', opacity=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ytS29030UWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cluster the locations?\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=20, n_jobs=-1)\n",
        "train['cluster'] = kmeans.fit_predict(train[['longitude', 'latitude']])\n",
        "test['cluster'] = kmeans.predict(test[['longitude', 'latitude']])\n",
        "px.scatter(train, x='longitude', y='latitude', color='cluster')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDGtTtCF4LDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.groupby('cluster').price.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6oiRJat4Yti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.catplot(x='cluster', y='price', data=train, kind='bar', color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wuxrner0UWc",
        "colab_type": "text"
      },
      "source": [
        "### Advice\n",
        "\n",
        "Do exploratory visualization for predictive modeling. Visualize the relationships between feature(s) and target.\n",
        "\n",
        "Do this with your training set, after splitting your data. \n",
        "\n",
        "Try [Seaborn \"Categorical estimate\" plots](https://seaborn.pydata.org/tutorial/categorical.html) for features with reasonably few unique values. (With too many unique values, the plot is unreadable.)\n",
        "\n",
        "- Categorical features. (If there are too many unique values, you can replace less frequent values with \"OTHER.\")\n",
        "- Numeric features. (If there are too many unique values, you can [bin with pandas cut / qcut functions](https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html?highlight=qcut#discretization-and-quantiling).)\n",
        "\n",
        "Try [Seaborn linear model plots](https://seaborn.pydata.org/tutorial/regression.html) with numeric features.\n",
        "\n",
        "Seaborn is nice because it includes confidence intervals to visualize uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gn3wjS617tF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiX6c09t0UWg",
        "colab_type": "text"
      },
      "source": [
        "# Do Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bps_olg9zQ4o",
        "colab_type": "text"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIcjv5Kx0UWh",
        "colab_type": "text"
      },
      "source": [
        "The previous assignment quoted Wikipedia on [Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering):\n",
        "\n",
        "> \"Some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\" — Pedro Domingos, [\"A Few Useful Things to Know about Machine Learning\"](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
        "\n",
        "> \"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied machine learning' is basically feature engineering.\" — Andrew Ng, [Machine Learning and AI via Brain simulations](https://forum.stanford.edu/events/2011/2011slides/plenary/2011plenaryNg.pdf) \n",
        "\n",
        "> Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. \n",
        "\n",
        "Pedro Domingos says, \"the most important factor is the **features used**.\"\n",
        "\n",
        "This includes not just **Feature Engineering** (making new features, representing features in new ways) but also **Feature Selection** (choosing which features to include and which to exclude).\n",
        "\n",
        "There are _many_ specific tools and techniques for feature selection.\n",
        "\n",
        "- Today we'll try [scikit-learn's `SelectKBest` transformer](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), for \"univariate, forward selection.\"\n",
        "- Next week we'll try another technique, [\"permutation importance\"](https://www.kaggle.com/dansbecker/permutation-importance)\n",
        "- If you want to explore even more options, here are some good resources!\n",
        "  - [scikit-learn's User Guide for Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
        "  - [mlxtend](http://rasbt.github.io/mlxtend/) library\n",
        "  - scikit-learn-contrib libraries: [boruta_py](https://github.com/scikit-learn-contrib/boruta_py) & [stability-selection](https://github.com/scikit-learn-contrib/stability-selection)\n",
        "  - [_Feature Engineering and Selection_](http://www.feat.engineering/) by Kuhn & Johnson.\n",
        "\n",
        "\n",
        "My general recommendation is:\n",
        "\n",
        "> Predictive accuracy on test sets is the criterion for how good the model is. — Leo Breiman, [\"Statistical Modeling: The Two Cultures\"](https://projecteuclid.org/download/pdf_1/euclid.ss/1009213726)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpvymlGL0UNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, engineer some more features to select from\n",
        "\n",
        "def engineer_features(df):\n",
        "    # What's the neighborhood, based on address or latitude & longitude?\n",
        "    clusters = pd.get_dummies(df['cluster'], prefix='cluster')\n",
        "    for col in clusters:\n",
        "        df[col] = clusters[col]\n",
        "        \n",
        "    # Does the apartment have a description?\n",
        "    df['description'] = df['description'].str.strip().fillna('')\n",
        "    df['has_description'] = df['description'] != ''\n",
        "\n",
        "    # How long is the description?\n",
        "    df['description_length'] = df['description'].str.len()\n",
        "\n",
        "    # How many total perks does each apartment have?\n",
        "    perk_cols = ['elevator', 'cats_allowed', 'hardwood_floors', 'dogs_allowed',\n",
        "                 'doorman', 'dishwasher', 'no_fee', 'laundry_in_building',\n",
        "                 'fitness_center', 'pre-war', 'laundry_in_unit', 'roof_deck',\n",
        "                 'outdoor_space', 'dining_room', 'high_speed_internet', 'balcony',\n",
        "                 'swimming_pool', 'new_construction', 'exclusive', 'terrace', \n",
        "                 'loft', 'garden_patio', 'common_outdoor_space', \n",
        "                 'wheelchair_access']\n",
        "    df['perk_count'] = df[perk_cols].sum(axis=1)\n",
        "\n",
        "    # Are cats or dogs allowed?\n",
        "    df['cats_or_dogs'] = (df['cats_allowed']==1) | (df['dogs_allowed']==1)\n",
        "\n",
        "    # Are cats and dogs allowed?\n",
        "    df['cats_and_dogs'] = (df['cats_allowed']==1) & (df['dogs_allowed']==1)\n",
        "\n",
        "    # Total number of rooms (beds + baths)\n",
        "    df['rooms'] = df['bedrooms'] + df['bathrooms']\n",
        "\n",
        "    return df\n",
        "\n",
        "    \n",
        "train = engineer_features(train)\n",
        "test = engineer_features(test)\n",
        "train.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBKy0Zzi0UWh",
        "colab_type": "text"
      },
      "source": [
        "### Can we try every possible feature combination?\n",
        "- https://en.wikipedia.org/wiki/Combination\n",
        "- https://docs.python.org/3/library/itertools.html#itertools.combinations\n",
        "- https://docs.python.org/3/library/math.html#math.factorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4sFHW5v0UWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many features do we have currently?\n",
        "target = 'price'\n",
        "features = train.columns.drop(target)\n",
        "len(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnfzNVmc0UWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many ways to choose 1 feature?\n",
        "from itertools import combinations\n",
        "len(list(combinations(features, 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjQTR2I00UWl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many ways to choose 2 features?\n",
        "len(list(combinations(features, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxwE2mto0UWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many ways to choose 3 features?\n",
        "len(list(combinations(features, 3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLPVb0510UWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How many ways to choose 1 to n features?\n",
        "\n",
        "from math import factorial\n",
        "\n",
        "def n_choose_k(n, k):\n",
        "    return factorial(n)/(factorial(k)*factorial(n-k))\n",
        "\n",
        "total = 0\n",
        "for k in range(1, len(features)+1):\n",
        "    total += n_choose_k(len(features), k)\n",
        "    \n",
        "print(f'{total:,}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQpQTtNwzZU2",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TwakmBi0UWs",
        "colab_type": "text"
      },
      "source": [
        "### Start simple & fast, with a subset of columns\n",
        "\n",
        "Just numeric columns with no missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7dGWjD-0UWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a5hRDlu0UWv",
        "colab_type": "text"
      },
      "source": [
        "### Univariate, Forward selection\n",
        "https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eX6foc10UWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yHOmHvG0UWw",
        "colab_type": "text"
      },
      "source": [
        "# Do one-hot encoding of categorical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk59EsKtz-Dd",
        "colab_type": "text"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyImpIPW0UWx",
        "colab_type": "text"
      },
      "source": [
        "### Which features are non-numeric?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQRQcpliM__f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf_QH_dJ0UWz",
        "colab_type": "text"
      },
      "source": [
        "### Check \"cardinality\" of non-numeric features\n",
        "\n",
        "[Cardinality](https://simple.wikipedia.org/wiki/Cardinality) means the number of unique values that a feature has:\n",
        "> In mathematics, the cardinality of a set means the number of its elements. For example, the set A = {2, 4, 6} contains 3 elements, and therefore A has a cardinality of 3. \n",
        "\n",
        "\"One-hot encoding\" adds a dimension for each unique value of each categorical feature. So, it may not be a good choice for \"high cardinality\" categoricals that have dozens, hundreds, or thousands of unique values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc3qW8sX0UWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mLfEvRQ0UW0",
        "colab_type": "text"
      },
      "source": [
        "### Explore `interest_level` feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLn3mhZ-0UW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqePupnU0UW2",
        "colab_type": "text"
      },
      "source": [
        "### Encode `interest_level` feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN1XakA80UW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHJoKTeI0UW4",
        "colab_type": "text"
      },
      "source": [
        "### Do one-hot encoding & Scale features, \n",
        "within a complete model fitting workflow.\n",
        "\n",
        "#### Why and how to scale features before fitting linear models\n",
        "\n",
        "Scikit-Learn User Guide, [Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "> Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
        "\n",
        "> The `preprocessing` module further provides a utility class `StandardScaler` that implements the `Transformer` API to compute the mean and standard deviation on a training set. The scaler instance can then be used on new data to transform it the same way it did on the training set.\n",
        "\n",
        "#### How to use encoders and scalers in scikit-learn\n",
        "- Use the **`fit_transform`** method on the **train** set\n",
        "- Use the **`transform`** method on the **validation / test** sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB6vPmYh0K99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}